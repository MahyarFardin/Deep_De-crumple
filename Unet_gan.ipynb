{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75abf935-944f-441b-89a2-add464cfb13b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d8ba55-4cff-4565-b721-ee5ff943d9ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    This is the control cell\n",
    "    resume model name: is the directory to resume from\n",
    "    resume epoch: is the epoch from that specific run that you want to resume\n",
    "    resume from last: should be true with resume parameter to resume from last run of current model\n",
    "    resume: resume from specific directory with sepcific epoch\n",
    "'''\n",
    "root_data_dir = \"maps/\"\n",
    "runs_dir = \"Runs/\"\n",
    "model_name = \"LR_0005_Disc_Factor_1_Gen_Factor_25_Truely_dynamic_disc_loss_BN_Everywhere\"\n",
    "resume_model_name = \"\"\n",
    "resume_epoch = 0\n",
    "model_dir = runs_dir + model_name + \"/\"\n",
    "if os.listdir(runs_dir).count(model_name) == 0:\n",
    "    os.mkdir(runs_dir + model_name)\n",
    "maps = os.listdir(root_data_dir)\n",
    "dmaps = [m for m in maps if m[0] == \"d\"]\n",
    "gmaps = [m for m in maps if m[0] == \"g\"]\n",
    "resume_from_last = False\n",
    "resume = False\n",
    "train_val_split_rate = 0.9\n",
    "n_crumples = len(dmaps)\n",
    "idxs = np.arange(n_crumples)\n",
    "np.random.shuffle(idxs)\n",
    "e_crumple_idxs = idxs[:int(n_crumples * train_val_split_rate)]\n",
    "f_crumple_idxs = idxs[int(n_crumples * train_val_split_rate):]\n",
    "Batch_size = 5\n",
    "Epochs = 150\n",
    "Val_Batch_size = Batch_size * 2\n",
    "total_chunks = 153\n",
    "images_per_chunk = 1000\n",
    "steps_per_epoch = int(images_per_chunk * train_val_split_rate) * total_chunks // Batch_size\n",
    "steps_per_val_epoch = int(images_per_chunk * (1 - train_val_split_rate)) * total_chunks // Val_Batch_size\n",
    "saved_models_list = os.listdir(runs_dir + model_name)\n",
    "key_func = lambda x:int(x.split(\"_\")[-1][:-3]) if x[-3:] == \".h5\" else 0\n",
    "saved_models_list.sort(key=key_func)\n",
    "if resume_from_last:\n",
    "    last_epoch = int(saved_models_list[-1].split(\"_\")[-1][:-3])\n",
    "else:\n",
    "    last_epoch = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa18897-6e48-4fdc-80c7-82d8df31e778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quick_deformation(crumple_id, img):\n",
    "    global dmaps, gmaps, root_data_dir\n",
    "    g_map = np.load(root_data_dir + gmaps[crumple_id])\n",
    "    d_map = np.load(root_data_dir + dmaps[crumple_id])\n",
    "    ret = np.zeros_like(img)\n",
    "    ret[...] = img[:, d_map[..., 0], d_map[..., 1], :] * np.expand_dims(g_map, 2)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5858cc97-6464-4058-926a-d9b19ed0323f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    data generator loades images in chunks of 1000\n",
    "    then feeds it by batch size to the network.\n",
    "    first we load image, then we normalize it between -+127.5 \n",
    "    then we apply deformation maps and re-normalize with same range\n",
    "'''\n",
    "def data_gen(training=True, batch_size=1):\n",
    "    global e_crumple_idxs, f_crumple_idxs, total_chunks, train_val_split_rate, images_per_chunk\n",
    "    if training:\n",
    "        crumple_idxs = e_crumple_idxs\n",
    "        start_batch = 0\n",
    "        end_batch = int(images_per_chunk * train_val_split_rate)\n",
    "    else:\n",
    "        crumple_idxs = f_crumple_idxs\n",
    "        start_batch = int(images_per_chunk * train_val_split_rate)\n",
    "        end_batch = images_per_chunk\n",
    "    while 1:\n",
    "        crumple_i = 0\n",
    "        for chunk_num in range(total_chunks):\n",
    "            for j in range(start_batch, end_batch, batch_size):\n",
    "                imgs = np.load(\"Images/\" + str(chunk_num) + \".npy\", mmap_mode=\"r\")\n",
    "                IMAGE_SHAPE = 180\n",
    "                imgs = np.expand_dims(imgs[j:j + batch_size], -1)\n",
    "                im = imgs[:,20:-20,20:-20]\n",
    "                im_norm = np.subtract(im, 127.5, dtype=np.float32)\n",
    "                cr_id = crumple_idxs[crumple_i]\n",
    "                cr_im = quick_deformation(cr_id, im)\n",
    "                cr_im_norm = np.subtract(cr_im,  127.5, dtype=np.float32)\n",
    "                yield cr_im_norm, im_norm\n",
    "                crumple_i = (crumple_i + 1) % len(crumple_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edb450ec-6f54-477c-aa44-7da0a13aeb37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    global resume_from_last, resume\n",
    "    if not resume_from_last and not resume:\n",
    "        FACTOR = 2.5\n",
    "        x_inp = tf.keras.layers.Input(shape=(180, 180, 1))\n",
    "        padding_layer = tf.keras.layers.ZeroPadding2D(padding=(2, 2))(x_inp) # 184\n",
    "        c1 = tf.keras.layers.Conv2D(int(24 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(padding_layer) # 184\n",
    "        c_dial_1 = tf.keras.layers.Conv2D(int(8 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(padding_layer)\n",
    "        c1_out = tf.keras.layers.Concatenate(axis = 3)([c1, c_dial_1])\n",
    "        c2 = tf.keras.layers.Conv2D(int(12 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c1_out) # 184\n",
    "        c_dial_2 = tf.keras.layers.Conv2D(int(4 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(c1_out)\n",
    "        c2_out = tf.keras.layers.Concatenate(axis = 3)([c2, c_dial_2])\n",
    "        bn0 = tf.keras.layers.BatchNormalization()(c2_out)\n",
    "\n",
    "        c3 = tf.keras.layers.Conv2D(int(12 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(bn0) # 184\n",
    "        c_dial_3 = tf.keras.layers.Conv2D(int(4 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(bn0)\n",
    "        c3_out = tf.keras.layers.Concatenate(axis = 3)([c3, c_dial_3])\n",
    "        c4 = tf.keras.layers.Conv2D(int(12 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c3_out) # 184\n",
    "        c_dial_4 = tf.keras.layers.Conv2D(int(4 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(c3_out)\n",
    "        c4_out = tf.keras.layers.Concatenate(axis = 3)([c4, c_dial_4])\n",
    "        bn1 = tf.keras.layers.BatchNormalization()(c4_out)\n",
    "\n",
    "        c5 = tf.keras.layers.Conv2D(int(12 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(bn1) # 184\n",
    "        c_dial_5 = tf.keras.layers.Conv2D(int(4 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(bn1)\n",
    "        c5_out = tf.keras.layers.Concatenate(axis = 3)([c5, c_dial_5])\n",
    "        p1 = tf.keras.layers.MaxPooling2D()(c5_out)\n",
    "        c5 = tf.keras.layers.Conv2D(int(16 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(p1) # 92\n",
    "        bn2 = tf.keras.layers.BatchNormalization()(c5)\n",
    "\n",
    "        c6 = tf.keras.layers.Conv2D(int(16 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(bn2) # 92\n",
    "        p2 = tf.keras.layers.MaxPooling2D()(c6)\n",
    "        c7 = tf.keras.layers.Conv2D(int(32 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(p2) # 46\n",
    "        bn3 = tf.keras.layers.BatchNormalization()(c7)\n",
    "        \n",
    "        c8 = tf.keras.layers.Conv2D(int(32 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(bn3) # 46\n",
    "        p3 = tf.keras.layers.MaxPooling2D()(c8)\n",
    "        c9 = tf.keras.layers.Conv2D(int(64 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(p3) # 23\n",
    "        bn4 = tf.keras.layers.BatchNormalization()(c9)\n",
    "        \n",
    "        c10 = tf.keras.layers.Conv2D(int(64 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(bn4) # 23\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        d11 = tf.keras.layers.Conv2D(int(64 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c10) # 23\n",
    "        bn5 = tf.keras.layers.BatchNormalization()(d11)\n",
    "        \n",
    "        d10 = tf.keras.layers.Conv2D(int(64 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(d11) # 23\n",
    "        u4 = tf.keras.layers.UpSampling2D()(d10)\n",
    "        cc4 = tf.keras.layers.Concatenate()([u4, c8])\n",
    "        d9 = tf.keras.layers.Conv2D(int(32 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c8) # 46\n",
    "        bn6 = tf.keras.layers.BatchNormalization()(d9)\n",
    "        \n",
    "        d8 = tf.keras.layers.Conv2D(int(32 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(bn6) # 46\n",
    "        u3 = tf.keras.layers.UpSampling2D()(d8)\n",
    "        cc3 = tf.keras.layers.Concatenate()([u3, c6])\n",
    "        d7 = tf.keras.layers.Conv2D(int(32 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(cc3) # 92\n",
    "        bn7 = tf.keras.layers.BatchNormalization()(d7)\n",
    "        \n",
    "        d6 = tf.keras.layers.Conv2D(int(16 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(bn7) # 92\n",
    "        u2 = tf.keras.layers.UpSampling2D()(d6)\n",
    "        cc2 = tf.keras.layers.Concatenate()([u2, c5_out])\n",
    "        d5 = tf.keras.layers.Conv2D(int(24 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(cc2) # 184\n",
    "        d_dial_5 = tf.keras.layers.Conv2D(int(8 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(cc2)\n",
    "        d5_out = tf.keras.layers.Concatenate(axis = 3)([d5, d_dial_5])\n",
    "        bn8 = tf.keras.layers.BatchNormalization()(d5_out)\n",
    "        \n",
    "        d4 = tf.keras.layers.Conv2D(int(12 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(bn8) # 184\n",
    "        d_dial_4 = tf.keras.layers.Conv2D(int(4 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(bn8)\n",
    "        d4_out = tf.keras.layers.Concatenate(axis = 3)([d4, d_dial_4])\n",
    "        bn9 = tf.keras.layers.BatchNormalization()(d4_out)\n",
    "        \n",
    "        d3 = tf.keras.layers.Conv2D(int(12 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(bn9) # 184\n",
    "        d_dial_3 = tf.keras.layers.Conv2D(int(4 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(bn9)\n",
    "        d3_out = tf.keras.layers.Concatenate(axis = 3)([d3, d_dial_3])\n",
    "        d2 = tf.keras.layers.Conv2D(int(8 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(d3_out) # 184\n",
    "        d_dial_2 = tf.keras.layers.Conv2D(int(4 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(d3_out)\n",
    "        d2_out = tf.keras.layers.Concatenate(axis = 3)([d2, d_dial_2])\n",
    "        d1 = tf.keras.layers.Conv2D(1, (3, 3), padding=\"same\", activation=None)(d2) # 184\n",
    "        crop_layer = tf.keras.layers.Cropping2D(cropping=((2, 2), (2, 2)))(d1) # 180\n",
    "        model = tf.keras.models.Model(inputs=[x_inp], outputs=[crop_layer])\n",
    "        # model.compile(optimizer=tf.keras.optimizers.Adam(0.0003), loss=mean_elastic_distance)\n",
    "    elif resume and not resume_from_last:\n",
    "        target = glob.glob(runs_dir + \"/\" + resume_model_name + \"/*\" + resume_epoch + \"*\")[0]\n",
    "        model = tf.keras.models.load_model(target)\n",
    "    else:\n",
    "        model = tf.keras.models.load_model(runs_dir + \"/\" + model_name + \"/\" + os.listdir(runs_dir + model_name)[-2], custom_objects={\"mean_elastic_distance\":mean_elastic_distance})\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c961dbef-e56f-4929-8316-1a1cefab5c56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    if not resume_from_last:\n",
    "        FACTOR = 1.0\n",
    "        x_inp = tf.keras.layers.Input(shape=(180, 180, 1))\n",
    "        padding_layer = tf.keras.layers.ZeroPadding2D(padding=(2, 2))(x_inp) # 184\n",
    "        c1 = tf.keras.layers.Conv2D(int(24 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(padding_layer) # 184\n",
    "        c_dial_1 = tf.keras.layers.Conv2D(int(8 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(padding_layer)\n",
    "        c1_out = tf.keras.layers.Concatenate(axis = 3)([c1, c_dial_1])\n",
    "        c2 = tf.keras.layers.Conv2D(int(12 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c1_out) # 184\n",
    "        c_dial_2 = tf.keras.layers.Conv2D(int(4 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(c1_out)\n",
    "        c2_out = tf.keras.layers.Concatenate(axis = 3)([c2, c_dial_2])\n",
    "        c3 = tf.keras.layers.Conv2D(int(12 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c2_out) # 184\n",
    "        c_dial_3 = tf.keras.layers.Conv2D(int(4 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(c2_out)\n",
    "        c3_out = tf.keras.layers.Concatenate(axis = 3)([c3, c_dial_3])\n",
    "        c4 = tf.keras.layers.Conv2D(int(12 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c3_out) # 184\n",
    "        c_dial_4 = tf.keras.layers.Conv2D(int(4 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(c3_out)\n",
    "        c4_out = tf.keras.layers.Concatenate(axis = 3)([c4, c_dial_4])\n",
    "        c5 = tf.keras.layers.Conv2D(int(12 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c4_out) # 184\n",
    "        c_dial_5 = tf.keras.layers.Conv2D(int(4 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(c4_out)\n",
    "        c5_out = tf.keras.layers.Concatenate(axis = 3)([c5, c_dial_5])\n",
    "        p1 = tf.keras.layers.MaxPooling2D()(c5_out)\n",
    "        c5 = tf.keras.layers.Conv2D(int(16 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(p1) # 92\n",
    "        c6 = tf.keras.layers.Conv2D(int(16 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c5) # 92\n",
    "        p2 = tf.keras.layers.MaxPooling2D()(c6)\n",
    "        c7 = tf.keras.layers.Conv2D(int(16 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(p2) # 46\n",
    "        c8 = tf.keras.layers.Conv2D(int(16 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c7) # 46\n",
    "        p3 = tf.keras.layers.MaxPooling2D()(c8)\n",
    "        c9 = tf.keras.layers.Conv2D(int(16 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(p3) # 23\n",
    "        c10 = tf.keras.layers.Conv2D(int(16 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c9) # 23\n",
    "        p4 = tf.keras.layers.MaxPooling2D()(c10)\n",
    "        c11 = tf.keras.layers.Conv2D(int(16 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(p4) # 12\n",
    "        c12 = tf.keras.layers.Conv2D(int(16 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c11) # 12\n",
    "        p5 = tf.keras.layers.MaxPooling2D()(c12)\n",
    "        c13 = tf.keras.layers.Conv2D(int(16 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(p5) # 12\n",
    "        c14 = tf.keras.layers.Conv2D(int(16 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c13) # 12\n",
    "        p6 = tf.keras.layers.MaxPooling2D()(c14)\n",
    "        c15 = tf.keras.layers.Conv2D(int(16 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(p6) # 6\n",
    "        c16 = tf.keras.layers.Conv2D(int(16 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c15) # 6\n",
    "        p7 = tf.keras.layers.GlobalMaxPooling2D()(c16)\n",
    "        out_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\")(p7)\n",
    "        model = tf.keras.models.Model(inputs=[x_inp], outputs=[out_layer])\n",
    "    else:\n",
    "        model = tf.keras.models.load_model(runs_dir + \"/\" + model_name + \"/\" + os.listdir(runs_dir + model_name)[-2])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1adb8f2b-908f-4079-86ef-387e2d725d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4565c3d-5759-4a65-85e5-0282ac19ae7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "discriminator = discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc6883a-e633-403e-8669-2b6d17062128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 180, 180, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " zero_padding2d (ZeroPadding2D)  (None, 184, 184, 1)  0          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 184, 184, 60  600         ['zero_padding2d[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 184, 184, 20  200         ['zero_padding2d[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 184, 184, 80  0           ['conv2d[0][0]',                 \n",
      "                                )                                 'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 184, 184, 30  21630       ['concatenate[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 184, 184, 10  7210        ['concatenate[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 184, 184, 40  0           ['conv2d_2[0][0]',               \n",
      "                                )                                 'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 184, 184, 40  160        ['concatenate_1[0][0]']          \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 184, 184, 30  10830       ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 184, 184, 10  3610        ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 184, 184, 40  0           ['conv2d_4[0][0]',               \n",
      "                                )                                 'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 184, 184, 30  10830       ['concatenate_2[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 184, 184, 10  3610        ['concatenate_2[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 184, 184, 40  0           ['conv2d_6[0][0]',               \n",
      "                                )                                 'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 184, 184, 40  160        ['concatenate_3[0][0]']          \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 184, 184, 30  10830       ['batch_normalization_1[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 184, 184, 10  3610        ['batch_normalization_1[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 184, 184, 40  0           ['conv2d_8[0][0]',               \n",
      "                                )                                 'conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 92, 92, 40)   0           ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 92, 92, 40)   14440       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 92, 92, 40)  160         ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 92, 92, 40)   14440       ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 46, 46, 40)  0           ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 46, 46, 80)   28880       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 46, 46, 80)  320         ['conv2d_12[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 46, 46, 80)   57680       ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 46, 46, 80)   57680       ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 46, 46, 80)  320         ['conv2d_18[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 46, 46, 80)   57680       ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 92, 92, 80)  0           ['conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 92, 92, 120)  0           ['up_sampling2d_1[0][0]',        \n",
      "                                                                  'conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 92, 92, 80)   86480       ['concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 92, 92, 80)  320         ['conv2d_20[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 92, 92, 40)   28840       ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 184, 184, 40  0          ['conv2d_21[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 184, 184, 80  0           ['up_sampling2d_2[0][0]',        \n",
      "                                )                                 'concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 184, 184, 60  43260       ['concatenate_7[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 184, 184, 20  14420       ['concatenate_7[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 184, 184, 80  0           ['conv2d_22[0][0]',              \n",
      "                                )                                 'conv2d_23[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 184, 184, 80  320        ['concatenate_8[0][0]']          \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 184, 184, 30  21630       ['batch_normalization_8[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 184, 184, 10  7210        ['batch_normalization_8[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 184, 184, 40  0           ['conv2d_24[0][0]',              \n",
      "                                )                                 'conv2d_25[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 184, 184, 40  160        ['concatenate_9[0][0]']          \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 184, 184, 30  10830       ['batch_normalization_9[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 184, 184, 10  3610        ['batch_normalization_9[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 184, 184, 40  0           ['conv2d_26[0][0]',              \n",
      "                                )                                 'conv2d_27[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 184, 184, 20  7220        ['concatenate_10[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 184, 184, 1)  181         ['conv2d_28[0][0]']              \n",
      "                                                                                                  \n",
      " cropping2d (Cropping2D)        (None, 180, 180, 1)  0           ['conv2d_30[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 529,361\n",
      "Trainable params: 528,401\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ea4f51f-6b01-4463-aac6-12182cec8ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 180, 180, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " zero_padding2d_1 (ZeroPadding2  (None, 184, 184, 1)  0          ['input_2[0][0]']                \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 184, 184, 24  240         ['zero_padding2d_1[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 184, 184, 8)  80          ['zero_padding2d_1[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 184, 184, 32  0           ['conv2d_31[0][0]',              \n",
      "                                )                                 'conv2d_32[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 184, 184, 12  3468        ['concatenate_12[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 184, 184, 4)  1156        ['concatenate_12[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 184, 184, 16  0           ['conv2d_33[0][0]',              \n",
      "                                )                                 'conv2d_34[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 184, 184, 12  1740        ['concatenate_13[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 184, 184, 4)  580         ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 184, 184, 16  0           ['conv2d_35[0][0]',              \n",
      "                                )                                 'conv2d_36[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 184, 184, 12  1740        ['concatenate_14[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 184, 184, 4)  580         ['concatenate_14[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenate)   (None, 184, 184, 16  0           ['conv2d_37[0][0]',              \n",
      "                                )                                 'conv2d_38[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 184, 184, 12  1740        ['concatenate_15[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 184, 184, 4)  580         ['concatenate_15[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 184, 184, 16  0           ['conv2d_39[0][0]',              \n",
      "                                )                                 'conv2d_40[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 92, 92, 16)  0           ['concatenate_16[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 92, 92, 16)   2320        ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 92, 92, 16)   2320        ['conv2d_41[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 46, 46, 16)  0           ['conv2d_42[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 46, 46, 16)   2320        ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 46, 46, 16)   2320        ['conv2d_43[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 23, 23, 16)  0           ['conv2d_44[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 23, 23, 16)   2320        ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 23, 23, 16)   2320        ['conv2d_45[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 11, 11, 16)  0           ['conv2d_46[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 11, 11, 16)   2320        ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 11, 11, 16)   2320        ['conv2d_47[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 5, 5, 16)    0           ['conv2d_48[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 5, 5, 16)     2320        ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 5, 5, 16)     2320        ['conv2d_49[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPooling2D)  (None, 2, 2, 16)    0           ['conv2d_50[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 2, 2, 16)     2320        ['max_pooling2d_8[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 2, 2, 16)     2320        ['conv2d_51[0][0]']              \n",
      "                                                                                                  \n",
      " global_max_pooling2d (GlobalMa  (None, 16)          0           ['conv2d_52[0][0]']              \n",
      " xPooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            17          ['global_max_pooling2d[0][0]']   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 39,761\n",
      "Trainable params: 39,761\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a875d871-e2ce-40fe-9dc7-6b2aecf98897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfd704b4-fea9-4e53-8be1-db15467b57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output, generator_output, ground_truth):\n",
    "    mse = tf.reduce_mean(tf.square(generator_output - ground_truth), axis=[1, 2, 3]) / 1000\n",
    "    mae = tf.reduce_mean(tf.abs(generator_output - ground_truth), axis=[1, 2, 3]) / 31.2\n",
    "    fooling_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    return tf.reduce_mean(fooling_loss + mae), fooling_loss, mae, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d4bd87e-8a99-46c5-8232-5861c79730ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return tf.reduce_mean(real_loss + fake_loss), real_loss, fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f58f10-b5d6-4404-a4f5-1553b6fb20cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(0.00005)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c1d2f-cd8f-4477-a1ae-6a119eb42aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 27540/27540 [3:52:04<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 29 / 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 27540/27540 [3:46:14<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 30 / 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 27540/27540 [3:49:26<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 31 / 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 27540/27540 [3:53:11<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 32 / 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 27540/27540 [3:58:38<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 33 / 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 27540/27540 [4:00:48<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 34 / 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 27540/27540 [4:04:04<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 35 / 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 27540/27540 [4:06:30<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 36 / 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 27540/27540 [4:05:43<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 37 / 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 27540/27540 [4:12:26<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 38 / 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 27540/27540 [4:18:09<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 39 / 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 27540/27540 [4:19:26<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 40 / 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 27540/27540 [4:17:00<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 41 / 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 27540/27540 [4:20:35<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 42 / 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 27540/27540 [4:23:12<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 43 / 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 27540/27540 [4:26:18<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 44 / 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 27540/27540 [4:29:27<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 45 / 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▊                                                                          | 662/27540 [06:34<4:28:10,  1.67it/s]"
     ]
    }
   ],
   "source": [
    "train_gen = data_gen(training=True, batch_size=Batch_size)\n",
    "gc.collect()\n",
    "gen_loss_history = []\n",
    "disc_loss_history = []\n",
    "flag_disc_train = True\n",
    "history_metric_gen_fooling_loss_hist = []\n",
    "history_metric_gen_mse_hist = []\n",
    "history_metric_gen_mae_hist = []\n",
    "history_metric_disc_real_loss_hist = []\n",
    "history_metric_disc_fake_loss_hist = []\n",
    "for e in range(last_epoch + 1, Epochs):\n",
    "    print(\"Epoch\", e + 1, \"/\", Epochs, \":\")\n",
    "    metric_gen_fooling_loss_hist = []\n",
    "    metric_gen_mse_hist = []\n",
    "    metric_gen_mae_hist = []\n",
    "    metric_disc_real_loss_hist = []\n",
    "    metric_disc_fake_loss_hist = []\n",
    "    this_epoch_disc_loss = []\n",
    "    for step in tqdm(range(int(total_chunks * 1000 * train_val_split_rate / Batch_size))):\n",
    "        noise, images = next(train_gen)\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            generated_images = generator(noise, training=True)\n",
    "            real_output = discriminator(images, training=True)\n",
    "            fake_output = discriminator(generated_images, training=True)\n",
    "            gen_loss, batch_gen_fooling_loss, batch_gen_mae, batch_gen_mse = generator_loss(fake_output, generated_images, images)#batch_gen_mse, cbatch_gen_mae = generator_loss(fake_output, generated_images, images)\n",
    "            disc_loss, batch_disc_real_loss, batch_disc_fake_loss = discriminator_loss(real_output, fake_output)\n",
    "        metric_gen_fooling_loss_hist.append(batch_gen_fooling_loss)\n",
    "        metric_gen_mse_hist.append(batch_gen_mse)\n",
    "        metric_gen_mae_hist.append(batch_gen_mae)\n",
    "        metric_disc_real_loss_hist.append(batch_disc_real_loss)\n",
    "        metric_disc_fake_loss_hist.append(batch_disc_fake_loss)\n",
    "        this_epoch_disc_loss.append(disc_loss)\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        zip1 = zip(gradients_of_generator, generator.trainable_variables)\n",
    "        generator_optimizer.apply_gradients(zip1)\n",
    "        if flag_disc_train: # Only update the discriminator every ten steps.\n",
    "            gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "            zip2 = zip(gradients_of_discriminator, discriminator.trainable_variables)\n",
    "            discriminator_optimizer.apply_gradients(zip2)\n",
    "    history_metric_gen_fooling_loss_hist.append(np.mean(metric_gen_fooling_loss_hist))\n",
    "    history_metric_gen_mse_hist.append(np.mean(metric_gen_mse_hist))\n",
    "    history_metric_gen_mae_hist.append(np.mean(metric_gen_mae_hist))\n",
    "    history_metric_disc_real_loss_hist.append(np.mean(metric_disc_real_loss_hist))\n",
    "    history_metric_disc_fake_loss_hist.append(np.mean(metric_disc_fake_loss_hist))\n",
    "    generator.save(model_dir + model_name + \"_gen_\" + str(e) + \".h5\")\n",
    "    discriminator.save(model_dir + model_name + \"_disc_\" + str(e) + \".h5\")\n",
    "    gen_loss_history.append(gen_loss)\n",
    "    disc_loss_history.append(disc_loss)\n",
    "    \n",
    "    if np.mean(this_epoch_disc_loss) < 0.1:\n",
    "        flag_disc_train = False\n",
    "    else:\n",
    "        flag_disc_train = True\n",
    "\n",
    "    plt.figure(figsize=(40, 30))\n",
    "    plt.subplot(7, 1, 1)\n",
    "    plt.plot(gen_loss_history)\n",
    "    plt.title(\"Generator Loss\")\n",
    "    plt.subplot(7, 1, 2)\n",
    "    plt.plot(disc_loss_history)\n",
    "    plt.title(\"Discriminator Loss\")\n",
    "    plt.subplot(7, 1, 3)\n",
    "    plt.plot(history_metric_gen_fooling_loss_hist)\n",
    "    plt.title(\"Generator Fooling Loss (Metric)\")\n",
    "    plt.subplot(7, 1, 4)\n",
    "    plt.plot(history_metric_gen_mse_hist)\n",
    "    plt.title(\"Generator MSE (Metric)\")\n",
    "    plt.subplot(7, 1, 5)\n",
    "    plt.plot(history_metric_gen_mae_hist)\n",
    "    plt.title(\"Generator MAE (Metric)\")\n",
    "    plt.subplot(7, 1, 6)\n",
    "    plt.plot(history_metric_disc_real_loss_hist)\n",
    "    plt.title(\"Discriminator Real Loss (Metric)\")\n",
    "    plt.subplot(7, 1, 7)\n",
    "    plt.plot(history_metric_disc_fake_loss_hist)\n",
    "    plt.title(\"Discriminator Fake Loss (Metric)\")\n",
    "    plt.savefig(model_dir + model_name + \"_status\" + \".png\")\n",
    "    plt.close()\n",
    "    \n",
    "    dg_tr = data_gen(training=True, batch_size=1)\n",
    "    dg_te = data_gen(training=False, batch_size=1)\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    fig, axs = plt.subplots(3, 8, figsize=(30, 12))\n",
    "    for i in range(4):\n",
    "        a, b = dg_tr.send(None)\n",
    "        axs[0, i].set_title(\"Train Input \" + str(i + 1))\n",
    "        axs[0, i].imshow(a[0], cmap='gray')\n",
    "        axs[0, i].axis('off')\n",
    "        axs[1, i].set_title(\"Train GT \" + str(i + 1))\n",
    "        axs[1, i].imshow(b[0], cmap='gray')\n",
    "        axs[1, i].axis('off')\n",
    "        axs[2, i].set_title(\"Train Output \" + str(i + 1))\n",
    "        axs[2, i].imshow(np.maximum(np.minimum(generator.predict(a, verbose=0)[0] + 127.5, 255), 0), cmap='gray')\n",
    "        axs[2, i].axis('off')\n",
    "        c, d = dg_te.send(None)\n",
    "        axs[0, 4 + i].set_title(\"Test Input \" + str(i + 1))\n",
    "        axs[0, 4 + i].imshow(c[0], cmap='gray')\n",
    "        axs[0, 4 + i].axis('off')\n",
    "        axs[1, 4 + i].set_title(\"Test GT \" + str(i + 1))\n",
    "        axs[1, 4 + i].imshow(d[0], cmap='gray')\n",
    "        axs[1, 4 + i].axis('off')\n",
    "        axs[2, 4 + i].set_title(\"Test Output \" + str(i + 1))\n",
    "        axs[2, 4 + i].imshow(np.maximum(np.minimum(generator.predict(c, verbose=0)[0] + 127.5, 255), 0), cmap='gray')\n",
    "        axs[2, 4 + i].axis('off')\n",
    "    fig.savefig(model_dir + model_name + \"_\" + str(e) + \".png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409dfcde-ad82-474d-95d3-103ec11edc82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0533d-0831-4fca-bcc7-36f14d971671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b591f89-cd78-4b31-8519-1a0d3584a831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c72e2c-7942-4052-a0bf-57e04ca72d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3d877-78ca-4ff1-a130-5c868895f194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849937e1-c1bb-41a9-ba23-8be88fe4f313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e91131-a7f9-425d-968d-7fc11797eb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35835ff2-5c51-4f37-894a-d42fdea59971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18647478-9925-46a9-9c25-ec7859c36690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0a02af-b472-483e-aa65-85e402d02411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b3410-3135-4a7c-8ea8-5383ad446017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c57aa-5d69-4b2d-b783-e9818e0a9f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e074957-1c04-48eb-8a9d-85ef4b3cf93b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ad080-6521-4608-8c96-ac60ecb098e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3906c119-a80a-4e99-9637-cf35ece20c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
